{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guoli\\AppData\\Local\\Temp\\ipykernel_13764\\3478394967.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.3)\n",
      "C:\\Users\\guoli\\AppData\\Local\\Temp\\ipykernel_13764\\3478394967.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\")  # Define um padrão caso não esteja no .env\n",
    "\n",
    "# Configuração do modelo de IA\n",
    "llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.3)\n",
    "\n",
    "# Memória resumida para manter o contexto sem gastar muitos tokens\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arandom = Agent(\n",
    "    role=\"Gerador de conceitos aleatorias\",\n",
    "    goal=\"Sua tarefa será falar palavras aleatórias, como animais, pessoas, paises, objetos, etc\",\n",
    "    backstory=f\"Você deve gerar palavras aleatórias para que o contador de historias possa adicionar em sua historia\",\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "Ahistory = Agent(\n",
    "    role=\"Contador de historas\",\n",
    "    goal=\"Sua tarefa será ouvir as palavras aleatórias do gerador de conceitos e criar histórias a partir deles, adicionando os elementos todos a uma mesma história\",\n",
    "    backstory=\"Você cria histórias, e a partir de conceitos aleatórios deve criar e adicionar esses conceitos em suas histórias\",\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def GetHistoryAIResponse(InputText):\n",
    "    taskHisotry = Task(\n",
    "    description=(\n",
    "        f\"Você deve adicionar o seguinte contexto em sua história: {InputText}\"\n",
    "    ),\n",
    "    agent=Ahistory, \n",
    "    memory=memory,\n",
    "    expected_output=\"Gere uma história somando os contextos apresentados a sua história, caso ainda não haja história comece com o contexto apresentado\" \n",
    "    )\n",
    "\n",
    "    crewBuyer = Crew(\n",
    "    agents=[Ahistory],\n",
    "    tasks=[taskHisotry]\n",
    "    )\n",
    "\n",
    "    return crewBuyer.kickoff()\n",
    "\n",
    "def GetRandomAIResponse():\n",
    "    taskRandom = Task(\n",
    "    description=(\n",
    "        \"Retorne uma palavra completamente aleatoria, podendo ser um pais, animal, objeto ou qualquer outra coisa, nunca retorne a mesma palavra duas vezes\"\n",
    "    ),\n",
    "    agent=Arandom, \n",
    "    memory=memory,\n",
    "    expected_output=\"Apenas uma palavra aleatória\"\n",
    "    )\n",
    "\n",
    "    crewSeller = Crew(\n",
    "    agents=[Arandom],\n",
    "    tasks=[taskRandom]\n",
    "    )\n",
    "\n",
    "    return crewSeller.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girafa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em uma vasta savana africana, onde o sol brilhava intensamente e as árvores se erguiam como torres verdes, vivia uma girafa chamada Gigi. Gigi era uma girafa curiosa e sonhadora, sempre em busca de novas aventuras. Um dia, enquanto caminhava entre as acácias, ela ouviu um rumor vindo de um grupo de animais reunidos em um círculo. Intrigada, Gigi se aproximou e viu que todos estavam falando sobre um misterioso lago que, segundo rumores, tinha o poder de realizar desejos.\n",
      "\n",
      "Movida pela curiosidade, Gigi decidiu que precisava encontrar esse lago mágico. Com seu longo pescoço, ela conseguia avistar as paisagens ao longe e, após horas de caminhada, avistou uma bela água cristalina refletindo o céu azul. Ao chegar ao lago, Gigi ficou maravilhada com a beleza do lugar. As flores ao redor dançavam com a brisa e o som da água era como uma melodia suave.\n",
      "\n",
      "Gigi se aproximou da margem e, olhando para a água, fez um pedido sincero: \"Eu desejo entender melhor os sentimentos dos meus amigos animais.\" Assim que as palavras deixaram seus lábios, uma onda de luz emanou do lago, e Gigi sentiu uma energia percorrer seu corpo. De repente, ela percebeu que podia ouvir os pensamentos e sentimentos dos outros animais ao seu redor.\n",
      "\n",
      "Com essa nova habilidade, Gigi começou a ajudar seus amigos. Ela ouviu a tristeza de um leão que se sentia solitário, a ansiedade de uma zebra que tinha medo de se perder e a alegria de um elefante que estava prestes a se tornar pai. Usando seu dom, Gigi organizou encontros entre os animais, ajudando-os a se entenderem melhor e a formarem laços mais fortes.\n",
      "\n",
      "A savana logo se transformou em um lugar mais harmonioso, onde todos se apoiavam e compartilhavam suas emoções. Gigi, a girafa que buscava aventuras, tornou-se a guardiã da amizade entre os animais. Ela aprendeu que, às vezes, o verdadeiro desejo não é apenas ser feliz, mas também fazer os outros felizes. E assim, a girafa e seus amigos viveram muitas outras aventuras, sempre unidos e compreendendo uns aos outros, enquanto o lago mágico continuava a brilhar sob o sol da savana.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinguim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em uma remota e gelada região da Antártica, havia um pinguim chamado Pip. Pip era um pinguim diferente dos outros; enquanto seus amigos passavam o dia deslizando pelo gelo e mergulhando nas águas frias em busca de peixes, ele sonhava em explorar o mundo além do horizonte branco. Ele sempre olhava para o céu e imaginava como seria voar como as aves que passavam por ali.\n",
      "\n",
      "Um dia, enquanto Pip estava em uma de suas reflexões, ele encontrou uma garrafa flutuando no mar. Curioso, ele a puxou para perto e, para sua surpresa, encontrou um mapa dentro dela. O mapa mostrava uma ilha misteriosa, repleta de tesouros e criaturas fantásticas. O coração de Pip disparou de emoção. Ele decidiu que precisava ir até lá, mesmo que isso significasse deixar sua colônia de pinguins.\n",
      "\n",
      "Com a ajuda de seus amigos, Pip começou a planejar a jornada. Eles construíram uma pequena jangada com pedaços de gelo e madeira que encontraram na praia. No dia da partida, Pip se despediu de sua família e amigos, prometendo voltar com histórias incríveis.\n",
      "\n",
      "A viagem foi cheia de desafios. Eles enfrentaram tempestades, ondas gigantes e até mesmo um grupo de focas curiosas que tentaram roubar sua comida. Mas Pip nunca desistiu. Ele usou sua inteligência e coragem para superar cada obstáculo, sempre sonhando com a ilha mágica que o aguardava.\n",
      "\n",
      "Finalmente, após dias de navegação, Pip e seus amigos avistaram a ilha. Era um lugar deslumbrante, coberto de flores coloridas e árvores frutíferas. Assim que desembarcaram, foram recebidos por criaturas incríveis: pássaros de cores vibrantes, coelhos falantes e até mesmo uma tartaruga sábia que os guiou pela ilha.\n",
      "\n",
      "Pip explorou cada canto, coletando histórias e experiências que nunca imaginou viver. Ele aprendeu sobre a importância da amizade, da coragem e da curiosidade. Quando finalmente decidiu que era hora de voltar para casa, Pip estava cheio de tesouros: não apenas ouro e joias, mas também histórias que contaria por toda a vida.\n",
      "\n",
      "Ao retornar à sua colônia, Pip foi recebido como um herói. Ele compartilhou suas aventuras, inspirando outros pinguins a sonhar e explorar. E assim, o pinguim que um dia apenas sonhou em voar, tornou-se uma lenda entre seus pares, mostrando que a verdadeira aventura está em seguir nossos sonhos, não importa quão distantes eles possam parecer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abacaxi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     RandomSpeaking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     resultado \u001b[38;5;241m=\u001b[39m \u001b[43mGetHistoryAIResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresultado\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(resultado)\n\u001b[0;32m     22\u001b[0m     RandomSpeaking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m, in \u001b[0;36mGetHistoryAIResponse\u001b[1;34m(InputText)\u001b[0m\n\u001b[0;32m     18\u001b[0m taskHisotry \u001b[38;5;241m=\u001b[39m Task(\n\u001b[0;32m     19\u001b[0m description\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocê deve adicionar o seguinte contexto em sua história: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mInputText\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m expected_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGere uma história somando os contextos apresentados a sua história, caso ainda não haja história comece com o contexto apresentado\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m crewBuyer \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[0;32m     28\u001b[0m agents\u001b[38;5;241m=\u001b[39m[Ahistory],\n\u001b[0;32m     29\u001b[0m tasks\u001b[38;5;241m=\u001b[39m[taskHisotry]\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcrewBuyer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\crew.py:558\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    555\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[1;32m--> 558\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[0;32m    560\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\crew.py:665\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[0;32m    664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\crew.py:767\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    764\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    766\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 767\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\agent.py:248\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    245\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    105\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_rpm_limit()\n\u001b[1;32m--> 140\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_llm_response(answer)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:201\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\crewai\\llm.py:252\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    249\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# --- 2) Make the completion call\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m response_message \u001b[38;5;241m=\u001b[39m cast(Choices, cast(ModelResponse, response)\u001b[38;5;241m.\u001b[39mchoices)[\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    255\u001b[0m ]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    256\u001b[0m text_response \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\litellm\\utils.py:978\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    976\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\litellm\\main.py:1599\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m## COMPLETION CALL\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1599\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[0;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[0;32m   1620\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[0;32m   1621\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   1622\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m   1623\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m   1624\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[0;32m   1625\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:634\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[1;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[0;32m    622\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    624\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_client\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     },\n\u001b[0;32m    631\u001b[0m )\n\u001b[0;32m    633\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m )\n\u001b[0;32m    642\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mmodel_call_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m headers\n\u001b[0;32m    643\u001b[0m stringified_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:145\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:435\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[1;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[0;32m    433\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    440\u001b[0m         headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:863\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    860\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    861\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    862\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\openai\\_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\openai\\_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\guoli\\Documents\\Insper\\PFE\\PFE-NTT\\venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1232\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Carregar variáveis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Usar as variáveis carregadas\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")  # Define um padrão caso não esteja no .env\n",
    "\n",
    "# Configuração do modelo de IA\n",
    "llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.3)\n",
    "\n",
    "# Simulação do diálogo\n",
    "RandomSpeaking = True\n",
    "\n",
    "for i in range(6):\n",
    "    if RandomSpeaking:\n",
    "        resultado = GetRandomAIResponse()\n",
    "        print(resultado)\n",
    "        RandomSpeaking = False\n",
    "    else:\n",
    "        resultado = GetHistoryAIResponse(resultado)\n",
    "        print(resultado)\n",
    "        RandomSpeaking = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativa reiterando a ideia inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Configura LLM e memória\n",
    "llm = ChatOpenAI(openai_api_key=api_key, model_name=model_name, temperature=0.3)\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Função que gera resposta para um “papel” (comprador ou vendedor, ou gerador de palavra/historiador)\n",
    "def gerar_resposta(role, objetivo, entrada_outro, memory, llm):\n",
    "    # Carrega o resumo atual\n",
    "    resumo = memory.load_memory_variables({})[\"history\"]\n",
    "    # Monta prompt\n",
    "    prompt = (\n",
    "        f\"Você é {role}.\\n\"\n",
    "        f\"Seu objetivo: {objetivo}\\n\\n\"\n",
    "        f\"Resumo da conversa até agora:\\n{resumo}\\n\\n\"\n",
    "    )\n",
    "    if entrada_outro:\n",
    "        prompt += f\"O outro disse: {entrada_outro}\\n\"\n",
    "    prompt += \"Responda agora:\\n\"\n",
    "\n",
    "    # Gera resposta\n",
    "    resposta = llm.predict(prompt)\n",
    "\n",
    "    # Salva no histórico\n",
    "    memory.save_context({\"input\": entrada_outro or \"\"}, {\"output\": resposta})\n",
    "\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guoli\\AppData\\Local\\Temp\\ipykernel_40148\\3692886920.py:16: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resposta = llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerador de Palavras Aleatórias: Canguru.\n",
      "\n",
      "Contador de Histórias: O outro disse: Canguru. \n",
      "\n",
      "O animal, com seu jeito curioso e ágil, pulou de um lado para o outro, atraindo a atenção de todos ao redor. As crianças, fascinadas, começaram a imitar os saltos do canguru, rindo e se divertindo. Mas, no meio da brincadeira, um deles notou algo brilhante no chão. \n",
      "\n",
      "\"Olhem!\", gritou, apontando para o objeto. Era uma pequena pedra preciosa, que refletia a luz do sol em cores vibrantes. O grupo parou imediatamente, intrigado. O que aquela pedra poderia significar? E por que estava ali, em meio à diversão? \n",
      "\n",
      "Enquanto todos se aproximavam, um velho sábio que passava pela trilha parou e disse: \"Essa pedra é um símbolo de sorte, mas também de responsabilidade. O que vocês farão com ela pode mudar o destino de todos.\" \n",
      "\n",
      "As crianças se entreolharam, sentindo o peso daquelas palavras. O que fazer com a sorte que acabavam de encontrar?\n",
      "\n",
      "Gerador de Palavras Aleatórias: Elefante.\n",
      "\n",
      "Contador de Histórias: Enquanto os crianças ponderavam sobre a responsabilidade que a pedra trazia, um enorme elefante apareceu de repente na clareira. Ele caminhava lentamente, suas grandes orelhas balançando suavemente com o vento. As crianças, encantadas, deixaram de lado a pedra por um momento e correram em direção ao majestoso animal. O elefante, com seus olhos gentis e sábios, parecia entender a curiosidade deles.\n",
      "\n",
      "Um dos meninos, chamado Lucas, aproximou-se cautelosamente e estendeu a mão, tocando a pele áspera do elefante. \"Você também tem sorte, não é?\" ele perguntou, admirado. O elefante, como se tivesse compreendido a pergunta, balançou a cabeça e soltou um suave barulho, como se estivesse rindo.\n",
      "\n",
      "Os outros crianças começaram a imitar o som do elefante, criando uma sinfonia de risadas e gritos de alegria. Mas, enquanto brincavam, a pedra brilhante no chão ainda chamava a atenção de alguns deles. A sabedoria do velho homem ecoava em suas mentes: a sorte pode ser um presente, mas também pode exigir escolhas difíceis.\n",
      "\n",
      "\"Vamos perguntar ao elefante o que ele acha da pedra!\" sugeriu uma menina chamada Sofia, com os olhos brilhando de empolgação. Todos concordaram, e juntos, se voltaram para o elefante, esperando que ele pudesse oferecer alguma orientação sobre o que fazer com a pedra que simbolizava tanto.\n",
      "\n",
      "Gerador de Palavras Aleatórias: Gato.\n",
      "\n",
      "Contador de Histórias: Enquanto os crianças se voltavam para o elefante em busca de sabedoria, um gato curioso apareceu de repente, saindo de trás de uma árvore próxima. Com seu andar elegante e olhar astuto, o gato se aproximou do grupo, atraído pela agitação e pela presença do grande animal. As crianças, agora fascinadas pelo novo visitante, começaram a rir e a brincar com o gato, que se esfregava nas pernas de Lucas, como se quisesse participar da conversa.\n",
      "\n",
      "Sofia, percebendo a presença do gato, teve uma ideia. \"E se o gato também tiver algo a nos ensinar sobre o nosso dilema com a pedra?\" Ela sugeriu, olhando para o felino que, com um movimento suave, começou a brincar com uma folha caída no chão. O gato parecia despreocupado, como se não tivesse a menor ideia das responsabilidades que a pedra trazia.\n",
      "\n",
      "Os olhos do grupo se iluminaram com a possibilidade de que o gato, com sua sabedoria felina, pudesse oferecer uma nova perspectiva. Lucas, ainda acariciando o elefante, chamou o gato para perto. \"O que você acha, amigo? A pedra é um presente ou um fardo?\" O gato, em resposta, parou de brincar e olhou diretamente para Lucas, como se estivesse ponderando a pergunta.\n",
      "\n",
      "Nesse momento, o elefante emitiu um som profundo e suave, como se estivesse apoiando a ideia de que tanto o gato quanto ele tinham algo a compartilhar. As crianças, agora unidas em torno do gato e do elefante, sentiam que estavam prestes a descobrir uma lição valiosa sobre a sorte, a responsabilidade e a amizade. O gato, com um movimento ágil, saltou para o colo de Sofia, como se estivesse pronto para dar sua resposta.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Loop de turnos\n",
    "n_vezes = 6\n",
    "mensagem_atual = \"\"\n",
    "for i in range(n_vezes):\n",
    "    if i % 2 == 0:\n",
    "        # Gerador de palavra\n",
    "        role = \"Gerador de Palavras Aleatórias\"\n",
    "        objetivo = \"Gerar uma palavra aleatória (animal, objeto, etc). Nunca repita palavras.\"\n",
    "    else:\n",
    "        # Contador de histórias\n",
    "        role = \"Contador de Histórias\"\n",
    "        objetivo = \"Continuar a história existente, incorporando a última palavra recebida.\"\n",
    "    # Gera a resposta\n",
    "    resposta = gerar_resposta(role, objetivo, mensagem_atual, memory, llm)\n",
    "    print(f\"{role}: {resposta}\\n\")\n",
    "    mensagem_atual = resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo com vendedor e comprador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buyer: Olá! Boa tarde! Estou procurando uma camisa branca para um casamento. Você poderia me mostrar as opções que vocês têm?\n",
      "\n",
      "Seller: Olá! Boa tarde! Que bom que você veio até aqui. Temos algumas opções de camisas brancas que podem ser perfeitas para o casamento. \n",
      "\n",
      "Aqui estão alguns modelos que temos:\n",
      "\n",
      "1. **Camisa Branca Clássica** - Feita de algodão, com um corte slim. Ideal para ocasiões formais. Preço: R$ 149,90.\n",
      "   \n",
      "2. **Camisa Branca com Detalhes em Renda** - Um pouco mais sofisticada, com detalhes sutis que dão um charme especial. Preço: R$ 179,90.\n",
      "\n",
      "3. **Camisa Branca de Linho** - Perfeita para climas mais quentes, é leve e confortável. Preço: R$ 159,90.\n",
      "\n",
      "4. **Camisa Branca com Estampa Discreta** - Um toque moderno, mas ainda assim elegante. Preço: R$ 169,90.\n",
      "\n",
      "Você tem algum estilo em mente ou alguma preferência específica? Posso trazer alguns modelos para você experimentar!\n",
      "\n",
      "Buyer: Olá! Boa tarde! Estou interessado nas opções que você mencionou. A camisa clássica parece uma boa escolha, mas eu também gostei da ideia da camisa com detalhes em renda. Você poderia me contar um pouco mais sobre a diferença entre elas em termos de conforto e estilo? E como elas se encaixam em um casamento?\n",
      "\n",
      "Seller: Olá! Boa tarde! Fico feliz que você tenha gostado das opções. Vamos lá!\n",
      "\n",
      "A camisa clássica de algodão é uma escolha excelente para um casamento. Ela é super confortável, respirável e tem um caimento muito bom. O algodão é um material que se adapta bem ao corpo, o que proporciona liberdade de movimento. Além disso, ela tem um visual atemporal, que combina com praticamente qualquer estilo de traje.\n",
      "\n",
      "Por outro lado, a camisa com detalhes em renda traz um toque mais sofisticado e elegante. A renda adiciona um elemento de textura e pode ser um ótimo destaque, especialmente em um evento como um casamento. Ela pode ser um pouco menos confortável que a de algodão, dependendo do design, mas o efeito visual é realmente encantador.\n",
      "\n",
      "Em termos de preços, a camisa clássica está por R$ 149,90, enquanto a camisa com detalhes em renda custa R$ 199,90. Ambas são ótimas opções, mas a escolha vai depender do estilo que você deseja transmitir no casamento. Você tem alguma preferência em relação ao tipo de tecido ou ao estilo que gostaria de seguir?\n",
      "\n",
      "Buyer: Vou levar a camisa clássica de algodão.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    model_name=model_name,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def gerar_resposta_inicial_buyer(memory):\n",
    "    \"\"\"\n",
    "    Instruções ESPECIAIS para o primeiro turno do Buyer,\n",
    "    a fim de garantir que ele aja como comprador desde o início.\n",
    "    \"\"\"\n",
    "    resumo = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Você é o Buyer (comprador).\n",
    "Esta é a PRIMEIRA FALA da conversa, presencial numa loja de shopping.\n",
    "Seu objetivo: Você quer comprar uma camisa branca para um casamento.\n",
    "Não se identifique como IA. Você deve iniciar a conversa como um comprador,\n",
    "pedindo para ver camisas ou perguntar se há camisas brancas.\n",
    "\n",
    "NÃO ofereça produtos ou fale \"temos\" camisas, pois você é o comprador, não o vendedor.\n",
    "\n",
    "Responda agora, fazendo uma saudação e perguntando sobre as camisas brancas:\n",
    "\"\"\".strip()\n",
    "\n",
    "    resposta = llm.predict(prompt)\n",
    "    memory.save_context({\"input\": \"\"}, {\"output\": resposta})\n",
    "    return resposta\n",
    "\n",
    "\n",
    "def gerar_resposta(role, objetivo, fala_anterior, memory, turno_atual, turno_final):\n",
    "    \"\"\"\n",
    "    Gera a resposta do Buyer ou Seller nos turnos intermediários (não final do Buyer).\n",
    "    \"\"\"\n",
    "    resumo = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Você é o {role}.\n",
    "Vocês estão numa loja de roupas em um shopping, conversando presencialmente.\n",
    "Seu objetivo: {objetivo}\n",
    "\n",
    "Resumo da conversa até agora:\n",
    "{resumo}\n",
    "\n",
    "A última fala do outro interlocutor foi:\n",
    "{fala_anterior}\n",
    "\n",
    "INSTRUÇÕES IMPORTANTES:\n",
    "1. Mantenha o tom e o contexto de uma conversa presencial.\n",
    "2. Se você for Buyer, aja como comprador (pergunte sobre camisas, preços etc.).\n",
    "3. Se você for Seller, aja como vendedor (ofereça camisas, apresente preços etc.).\n",
    "4. Não se identifique como IA.\n",
    "\n",
    "Estamos no turno {turno_atual} de {turno_final}. \n",
    "Se não for o último turno do Buyer, apenas continue a conversa naturalmente, \n",
    "sem finalizar a compra.\n",
    "\n",
    "Responda agora:\n",
    "\"\"\".strip()\n",
    "\n",
    "    resposta = llm.predict(prompt)\n",
    "    memory.save_context({\"input\": fala_anterior}, {\"output\": resposta})\n",
    "    return resposta\n",
    "\n",
    "\n",
    "def gerar_resposta_final_buyer(fala_anterior, memory):\n",
    "    \"\"\"\n",
    "    Último turno do Buyer: decide afirmativamente (comprar ou não comprar)\n",
    "    de forma breve e clara (sem perguntas).\n",
    "    \"\"\"\n",
    "    resumo = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Você é o Buyer.\n",
    "Agora é o último turno da conversa. Decida se vai comprar a camisa.\n",
    "Se for comprar, use uma frase afirmativa (ex: \"Vou levar...\").\n",
    "Se não for comprar, use uma frase clara de recusa (ex: \"Não vou levar...\").\n",
    "Não faça perguntas. Seja breve. Não se identifique como IA.\n",
    "\n",
    "Resumo da conversa até agora:\n",
    "{resumo}\n",
    "\n",
    "A última fala do outro interlocutor foi:\n",
    "{fala_anterior}\n",
    "\n",
    "Responda agora, encerrando a conversa com a decisão:\n",
    "\"\"\".strip()\n",
    "\n",
    "    resposta = llm.predict(prompt)\n",
    "    memory.save_context({\"input\": fala_anterior}, {\"output\": resposta})\n",
    "    return resposta\n",
    "\n",
    "\n",
    "def main():\n",
    "    buyer_role = \"Buyer\"\n",
    "    buyer_obj = (\n",
    "        \"Você quer comprar uma camisa branca para um casamento. \"\n",
    "        \"Nos turnos iniciais, avalie opções. No último turno, \"\n",
    "        \"decida de forma afirmativa ou negativa, sem perguntas.\"\n",
    "    )\n",
    "\n",
    "    seller_role = \"Seller\"\n",
    "    seller_obj = (\n",
    "        \"Você é vendedor de camisas. Ofereça modelos, preços e tente vender.\"\n",
    "    )\n",
    "\n",
    "    # Defina quantos turnos quiser, desde que seja ímpar (para o Buyer terminar).\n",
    "    turno_final = 5\n",
    "    fala_anterior = \"\"\n",
    "\n",
    "    for turno_atual in range(1, turno_final + 1):\n",
    "        if turno_atual == 1:\n",
    "            # Primeiro turno, e é a vez do Buyer (porque 1 é ímpar).\n",
    "            # Então chamamos a função especial inicial do Buyer.\n",
    "            resposta = gerar_resposta_inicial_buyer(memory)\n",
    "            print(f\"{buyer_role}: {resposta}\\n\")\n",
    "            fala_anterior = resposta\n",
    "        \n",
    "        elif turno_atual == turno_final and (turno_atual % 2 == 1):\n",
    "            # Último turno e é a vez do Buyer => decisão final\n",
    "            resposta = gerar_resposta_final_buyer(fala_anterior, memory)\n",
    "            print(f\"{buyer_role}: {resposta}\\n\")\n",
    "        \n",
    "        else:\n",
    "            # Turnos normais (2..(turno_final-1)), ou se fosse a vez do Seller\n",
    "            if turno_atual % 2 == 1:\n",
    "                # Buyer\n",
    "                resposta = gerar_resposta(\n",
    "                    buyer_role, \n",
    "                    buyer_obj, \n",
    "                    fala_anterior, \n",
    "                    memory, \n",
    "                    turno_atual, \n",
    "                    turno_final\n",
    "                )\n",
    "                print(f\"{buyer_role}: {resposta}\\n\")\n",
    "                fala_anterior = resposta\n",
    "            else:\n",
    "                # Seller\n",
    "                resposta = gerar_resposta(\n",
    "                    seller_role, \n",
    "                    seller_obj, \n",
    "                    fala_anterior, \n",
    "                    memory, \n",
    "                    turno_atual, \n",
    "                    turno_final\n",
    "                )\n",
    "                print(f\"{seller_role}: {resposta}\\n\")\n",
    "                fala_anterior = resposta\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de uso de langchain mais profundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Você é o Buyer (comprador) em uma loja de roupas num shopping.\n",
      "Você NUNCA deve se identificar como IA.\n",
      "\n",
      "Contexto da conversa até agora (memória compartilhada):\n",
      "[]\n",
      "\n",
      "Human: \n",
      "Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\n",
      "\n",
      "(Seller não falou nada ainda)\n",
      "\n",
      "Como Buyer:\n",
      "1. Se este for seu último turno, dê uma resposta final (decisão de comprar ou não) e não faça perguntas.\n",
      "2. Se não for o último turno, continue a conversa naturalmente.\n",
      "3. Não se identifique como IA.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Buyer (turno 1): Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Você é o Seller (vendedor) em uma loja de roupas num shopping.\n",
      "Você NUNCA deve se identificar como IA.\n",
      "\n",
      "Contexto da conversa até agora (memória compartilhada):\n",
      "[HumanMessage(content='Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\\n\\n(Seller não falou nada ainda)', additional_kwargs={}, response_metadata={}), AIMessage(content='Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "Human: \n",
      "Objetivo do Seller: Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\n",
      "Última fala do Buyer: Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?\n",
      "\n",
      "Como Seller:\n",
      "1. Mantenha o tom de conversa presencial.\n",
      "2. Tente vender camisas (apresente modelos, preços etc.).\n",
      "3. Não se identifique como IA.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Seller (turno 2): Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\n",
      "\n",
      "1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\n",
      "\n",
      "2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\n",
      "\n",
      "3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\n",
      "\n",
      "Você tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Você é o Buyer (comprador) em uma loja de roupas num shopping.\n",
      "Você NUNCA deve se identificar como IA.\n",
      "\n",
      "Contexto da conversa até agora (memória compartilhada):\n",
      "[HumanMessage(content='Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\\n\\n(Seller não falou nada ainda)', additional_kwargs={}, response_metadata={}), AIMessage(content='Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Objetivo do Seller: Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\\nÚltima fala do Buyer: Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={}), AIMessage(content='Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\\n\\n1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\\n\\n2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\\n\\n3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\\n\\nVocê tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "Human: \n",
      "Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\n",
      "Última fala do Seller: Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\n",
      "\n",
      "1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\n",
      "\n",
      "2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\n",
      "\n",
      "3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\n",
      "\n",
      "Você tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!\n",
      "\n",
      "Como Buyer:\n",
      "1. Se este for seu último turno, dê uma resposta final (decisão de comprar ou não) e não faça perguntas.\n",
      "2. Se não for o último turno, continue a conversa naturalmente.\n",
      "3. Não se identifique como IA.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Buyer (turno 3): Estou pensando na camisa branca clássica, pois parece ser a mais adequada para um casamento. O corte slim fit e o tecido de algodão são exatamente o que eu procuro. Você poderia me trazer essa camisa para eu experimentar? E tem outras cores ou tamanhos disponíveis?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Você é o Seller (vendedor) em uma loja de roupas num shopping.\n",
      "Você NUNCA deve se identificar como IA.\n",
      "\n",
      "Contexto da conversa até agora (memória compartilhada):\n",
      "[HumanMessage(content='Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\\n\\n(Seller não falou nada ainda)', additional_kwargs={}, response_metadata={}), AIMessage(content='Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Objetivo do Seller: Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\\nÚltima fala do Buyer: Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={}), AIMessage(content='Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\\n\\n1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\\n\\n2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\\n\\n3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\\n\\nVocê tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\\nÚltima fala do Seller: Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\\n\\n1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\\n\\n2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\\n\\n3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\\n\\nVocê tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!', additional_kwargs={}, response_metadata={}), AIMessage(content='Estou pensando na camisa branca clássica, pois parece ser a mais adequada para um casamento. O corte slim fit e o tecido de algodão são exatamente o que eu procuro. Você poderia me trazer essa camisa para eu experimentar? E tem outras cores ou tamanhos disponíveis?', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "Human: \n",
      "Objetivo do Seller: Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\n",
      "Última fala do Buyer: Estou pensando na camisa branca clássica, pois parece ser a mais adequada para um casamento. O corte slim fit e o tecido de algodão são exatamente o que eu procuro. Você poderia me trazer essa camisa para eu experimentar? E tem outras cores ou tamanhos disponíveis?\n",
      "\n",
      "Como Seller:\n",
      "1. Mantenha o tom de conversa presencial.\n",
      "2. Tente vender camisas (apresente modelos, preços etc.).\n",
      "3. Não se identifique como IA.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Seller (turno 4): Claro! Vou trazer a camisa branca clássica para você experimentar. Ela realmente é uma ótima escolha para um casamento. \n",
      "\n",
      "Quanto às opções, temos ela disponível nos tamanhos P, M, G e GG. Além disso, temos também outras cores, como azul claro e cinza, se você quiser dar uma olhada. \n",
      "\n",
      "Vou pegar a camisa branca clássica em tamanho M e, se você quiser, posso trazer também a azul clara para você comparar. O que acha?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Você é o Buyer (comprador) em uma loja de roupas num shopping.\n",
      "Você NUNCA deve se identificar como IA.\n",
      "\n",
      "Contexto da conversa até agora (memória compartilhada):\n",
      "[HumanMessage(content='Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\\n\\n(Seller não falou nada ainda)', additional_kwargs={}, response_metadata={}), AIMessage(content='Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Objetivo do Seller: Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\\nÚltima fala do Buyer: Oi! Estou procurando uma camisa branca para um casamento. Você pode me ajudar? Quais opções vocês têm disponíveis? E qual é o estilo das camisas que vocês oferecem?', additional_kwargs={}, response_metadata={}), AIMessage(content='Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\\n\\n1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\\n\\n2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\\n\\n3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\\n\\nVocê tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\\nÚltima fala do Seller: Claro! Fico feliz em ajudar você a encontrar a camisa branca perfeita para o casamento. Temos algumas opções bem legais por aqui.\\n\\n1. **Camisa Branca Clássica**: Esta é uma camisa de algodão 100%, com um corte slim fit. É ideal para eventos formais e custa R$ 149,90.\\n\\n2. **Camisa Branca com Detalhes em Renda**: Se você quer algo um pouco mais sofisticado, temos essa opção com detalhes em renda nas mangas. Ela custa R$ 179,90 e é perfeita para dar um toque especial.\\n\\n3. **Camisa Branca com Estampa Sutil**: Para um visual mais descontraído, temos uma camisa com uma estampa bem leve, quase imperceptível. Essa sai por R$ 129,90.\\n\\nVocê tem algum estilo específico em mente? Ou alguma preferência quanto ao tipo de tecido? Posso trazer algumas opções para você experimentar!', additional_kwargs={}, response_metadata={}), AIMessage(content='Estou pensando na camisa branca clássica, pois parece ser a mais adequada para um casamento. O corte slim fit e o tecido de algodão são exatamente o que eu procuro. Você poderia me trazer essa camisa para eu experimentar? E tem outras cores ou tamanhos disponíveis?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Objetivo do Seller: Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\\nÚltima fala do Buyer: Estou pensando na camisa branca clássica, pois parece ser a mais adequada para um casamento. O corte slim fit e o tecido de algodão são exatamente o que eu procuro. Você poderia me trazer essa camisa para eu experimentar? E tem outras cores ou tamanhos disponíveis?', additional_kwargs={}, response_metadata={}), AIMessage(content='Claro! Vou trazer a camisa branca clássica para você experimentar. Ela realmente é uma ótima escolha para um casamento. \\n\\nQuanto às opções, temos ela disponível nos tamanhos P, M, G e GG. Além disso, temos também outras cores, como azul claro e cinza, se você quiser dar uma olhada. \\n\\nVou pegar a camisa branca clássica em tamanho M e, se você quiser, posso trazer também a azul clara para você comparar. O que acha?', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "Human: \n",
      "Objetivo do Buyer: Você quer comprar uma camisa branca para um casamento. Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\n",
      "Última fala do Seller: Claro! Vou trazer a camisa branca clássica para você experimentar. Ela realmente é uma ótima escolha para um casamento. \n",
      "\n",
      "Quanto às opções, temos ela disponível nos tamanhos P, M, G e GG. Além disso, temos também outras cores, como azul claro e cinza, se você quiser dar uma olhada. \n",
      "\n",
      "Vou pegar a camisa branca clássica em tamanho M e, se você quiser, posso trazer também a azul clara para você comparar. O que acha?\n",
      "\n",
      "Como Buyer:\n",
      "1. Se este for seu último turno, dê uma resposta final (decisão de comprar ou não) e não faça perguntas.\n",
      "2. Se não for o último turno, continue a conversa naturalmente.\n",
      "3. Não se identifique como IA.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Buyer (turno 5): Ótimo! A camisa branca clássica em tamanho M parece perfeita. E sim, por que não trazer a azul clara também? Assim posso comparar as duas. Obrigado pela ajuda!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4\")\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    model_name=model_name,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Memória de conversação compartilhada\n",
    "# 'memory_key=\"history\"' define que o histórico será injetado na variável {history}\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Prompts e Chains do BUYER\n",
    "# ------------------------------------------------------------------------------\n",
    "buyer_system_template = \"\"\"\n",
    "Você é o Buyer (comprador) em uma loja de roupas num shopping.\n",
    "Você NUNCA deve se identificar como IA.\n",
    "\n",
    "Contexto da conversa até agora (memória compartilhada):\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "buyer_human_template = \"\"\"\n",
    "{user_input}\n",
    "\n",
    "Como Buyer:\n",
    "1. Se este for seu último turno, dê uma resposta final (decisão de comprar ou não) e não faça perguntas.\n",
    "2. Se não for o último turno, continue a conversa naturalmente.\n",
    "3. Não se identifique como IA.\n",
    "\"\"\"\n",
    "\n",
    "buyer_system_msg = SystemMessagePromptTemplate.from_template(buyer_system_template)\n",
    "buyer_human_msg = HumanMessagePromptTemplate.from_template(buyer_human_template)\n",
    "\n",
    "buyer_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"history\", \"user_input\"],  # APENAS essas duas\n",
    "    messages=[buyer_system_msg, buyer_human_msg]\n",
    ")\n",
    "\n",
    "buyer_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=buyer_prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Prompts e Chains do SELLER\n",
    "# ------------------------------------------------------------------------------\n",
    "seller_system_template = \"\"\"\n",
    "Você é o Seller (vendedor) em uma loja de roupas num shopping.\n",
    "Você NUNCA deve se identificar como IA.\n",
    "\n",
    "Contexto da conversa até agora (memória compartilhada):\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "seller_human_template = \"\"\"\n",
    "{user_input}\n",
    "\n",
    "Como Seller:\n",
    "1. Mantenha o tom de conversa presencial.\n",
    "2. Tente vender camisas (apresente modelos, preços etc.).\n",
    "3. Não se identifique como IA.\n",
    "\"\"\"\n",
    "\n",
    "seller_system_msg = SystemMessagePromptTemplate.from_template(seller_system_template)\n",
    "seller_human_msg = HumanMessagePromptTemplate.from_template(seller_human_template)\n",
    "\n",
    "seller_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"history\", \"user_input\"],  # APENAS essas duas\n",
    "    messages=[seller_system_msg, seller_human_msg]\n",
    ")\n",
    "\n",
    "seller_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=seller_prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Função principal para orquestrar os turnos\n",
    "# ------------------------------------------------------------------------------\n",
    "def main():\n",
    "    buyer_obj = (\n",
    "        \"Você quer comprar uma camisa branca para um casamento. \"\n",
    "        \"Pergunte detalhes, analise opções e, no último turno, decida se vai levar ou não.\"\n",
    "    )\n",
    "    seller_obj = \"Você é um vendedor de camisas, tentando vender modelos brancos (ou similares).\"\n",
    "\n",
    "    # Número total de turnos (use ímpar para o Buyer encerrar)\n",
    "    total_turns = 5\n",
    "\n",
    "    # Guardamos aqui o texto da fala mais recente\n",
    "    buyer_utterance = \"\"\n",
    "    seller_utterance = \"\"\n",
    "\n",
    "    for turn in range(1, total_turns + 1):\n",
    "        if turn % 2 == 1:\n",
    "            # Turnos ímpares -> Buyer fala\n",
    "            # Montamos uma única string de entrada: user_input\n",
    "            if turn == 1:\n",
    "                # Primeiro turno do Buyer (pode ser algo bem inicial)\n",
    "                # Se quiser, já encaixa \"seller_utterance\" se tiver algo\n",
    "                user_input = f\"Objetivo do Buyer: {buyer_obj}\\n\\n(Seller não falou nada ainda)\"\n",
    "            else:\n",
    "                user_input = f\"Objetivo do Buyer: {buyer_obj}\\nÚltima fala do Seller: {seller_utterance}\"\n",
    "\n",
    "            response = buyer_chain.run(user_input=user_input)\n",
    "            print(f\"Buyer (turno {turn}): {response}\\n\")\n",
    "            buyer_utterance = response\n",
    "\n",
    "        else:\n",
    "            # Turnos pares -> Seller fala\n",
    "            user_input = f\"Objetivo do Seller: {seller_obj}\\nÚltima fala do Buyer: {buyer_utterance}\"\n",
    "            response = seller_chain.run(user_input=user_input)\n",
    "            print(f\"Seller (turno {turn}): {response}\\n\")\n",
    "            seller_utterance = response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de usuario humano conversando com agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bem-vindo ao chat! Você é o BUYER ===\n",
      "Digite 'exit' ou 'quit' para encerrar.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Você é o Seller (vendedor) em uma loja de roupas num shopping.\n",
      "Você NUNCA deve se identificar como IA.\n",
      "\n",
      "Contexto da conversa até agora (memória compartilhada):\n",
      "[]\n",
      "\n",
      "Human: \n",
      "Objetivo do Seller: Você é um vendedor de loja de roupas, tentando vender o que o comprador está buscando, mas se for um produto onde não vende em loja de roupa, voce não vai ter o que ele busca.\n",
      "Última fala do Buyer: ola, gostaria de um iphone\n",
      "\n",
      "Como Seller:\n",
      "1. Mantenha o tom de conversa presencial.\n",
      "2. Tente vender camisas (apresente modelos, preços etc.).\n",
      "3. Não se identifique como IA.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Seller: Olá! Que bom que você passou aqui na loja! Infelizmente, não temos iPhones, mas posso te ajudar a encontrar uma camisa incrível. Temos algumas novidades que acabaram de chegar. \n",
      "\n",
      "Olha só, temos uma camisa de algodão super confortável, disponível em várias cores, por apenas R$ 89,90. Também temos uma opção de camisa jeans que está super em alta, por R$ 99,90. \n",
      "\n",
      "Você está procurando algo específico em relação a estilo ou cor? Posso te mostrar algumas opções!\n",
      "\n",
      "Parece que a conversa foi encerrada (detecção pelo LLM).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4\")\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    model_name=model_name,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Memória de conversação\n",
    "# -------------------------------\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Chain para verificar se deve encerrar\n",
    "# -------------------------------\n",
    "check_end_prompt_template = \"\"\"\n",
    "Você é um classificador de intenção de encerramento de conversa.\n",
    "O usuário acabou de dizer: \"{last_user_message}\"\n",
    "\n",
    "- Se o usuário aparentemente JÁ CONCLUIU a compra, se despediu, não quer mais nada ou encerrou,\n",
    "  responda exatamente com \"NAO_CONTINUAR\" (sem aspas).\n",
    "- Caso contrário, responda exatamente \"CONTINUAR\".\n",
    "- NÃO escreva nada além de \"CONTINUAR\" ou \"NAO_CONTINUAR\".\n",
    "\"\"\"\n",
    "\n",
    "check_end_prompt = PromptTemplate(\n",
    "    input_variables=[\"last_user_message\"],\n",
    "    template=check_end_prompt_template\n",
    ")\n",
    "\n",
    "check_end_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=check_end_prompt,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def should_end_conversation(buyer_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Retorna True se a conversa deve encerrar, segundo o LLM.\n",
    "    \"\"\"\n",
    "    result = check_end_chain.run(last_user_message=buyer_text)\n",
    "    # Remove espaços, quebras de linha, etc.\n",
    "    result_clean = result.strip().upper()\n",
    "    return \"NAO_CONTINUAR\" in result_clean\n",
    "\n",
    "# -------------------------------\n",
    "# Chain do Seller\n",
    "# -------------------------------\n",
    "seller_system_template = \"\"\"\n",
    "Você é o Seller (vendedor) em uma loja de roupas num shopping.\n",
    "Você NUNCA deve se identificar como IA.\n",
    "\n",
    "Contexto da conversa até agora (memória compartilhada):\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "seller_human_template = \"\"\"\n",
    "{user_input}\n",
    "\n",
    "Como Seller:\n",
    "1. Mantenha o tom de conversa presencial.\n",
    "2. Tente vender roupas que o usuário busca (apresente modelos, preços etc.).\n",
    "3. Não se identifique como IA.\n",
    "\"\"\"\n",
    "\n",
    "seller_system_msg = SystemMessagePromptTemplate.from_template(seller_system_template)\n",
    "seller_human_msg = HumanMessagePromptTemplate.from_template(seller_human_template)\n",
    "\n",
    "seller_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"history\", \"user_input\"],\n",
    "    messages=[seller_system_msg, seller_human_msg]\n",
    ")\n",
    "\n",
    "seller_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=seller_prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Loop Principal\n",
    "# -------------------------------\n",
    "def main():\n",
    "    print(\"=== Bem-vindo ao chat! Você é o BUYER ===\", flush=True)\n",
    "    print(\"Digite 'exit' ou 'quit' para encerrar.\\n\", flush=True)\n",
    "\n",
    "    seller_obj = \"Você é um vendedor de loja de roupas, tentando vender o que o comprador está buscando, mas se for um produto onde não vende em loja de roupa, voce não vai ter o que ele busca.\"\n",
    "\n",
    "    while True:\n",
    "        buyer_input = input(\"Você (Buyer): \")\n",
    "        if buyer_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Encerrando o chat manualmente.\", flush=True)\n",
    "            break\n",
    "        \n",
    "        # Verifica se o LLM acha que deve encerrar (ex.: \"Vou levar, obrigado\", etc.)\n",
    "        if should_end_conversation(buyer_input):\n",
    "            print(\"Parece que a conversa foi encerrada (detecção pelo LLM).\", flush=True)\n",
    "            break\n",
    "\n",
    "        # Se não encerrar, passamos para o Seller responder.\n",
    "        user_input = (\n",
    "            f\"Objetivo do Seller: {seller_obj}\\n\"\n",
    "            f\"Última fala do Buyer: {buyer_input}\"\n",
    "        )\n",
    "        seller_response = seller_chain.run(user_input=user_input)\n",
    "        print(f\"Seller: {seller_response}\\n\", flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro prompt do usuário e depois conversa entre 2 agentes, com um agente para parar a conversa se identificar que eles cehgaram em uma compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversa: Buyer vs Seller, com Controller ===\n",
      "\n",
      "\n",
      "Buyer (turno 1): Bom dia, estou buscando uma camisa branca para um casamento que estou indo, ja tenho o terno e calca, so preciso da camisa por favor\n",
      "\n",
      "Seller (turno 2): Bom dia! Que ótimo que você já tem o terno e a calça. Temos algumas opções de camisas brancas que podem combinar perfeitamente com o seu look. \n",
      "\n",
      "Temos uma camisa social de algodão, que custa R$ 129,90, e é bem confortável e elegante. Também temos uma opção de camisa de linho, que é um pouco mais leve e fresca, ideal para eventos mais formais, por R$ 149,90. \n",
      "\n",
      "Você tem alguma preferência em relação ao tecido ou ao estilo da camisa?\n",
      "\n",
      "Buyer (turno 3): Estou em dúvida entre as duas opções. A camisa de algodão parece ser bem confortável, mas a de linho parece mais fresca para um evento formal. Você poderia me dizer qual delas tem um caimento melhor? E elas estão disponíveis em diferentes tamanhos?\n",
      "\n",
      "Seller (turno 4): Claro! Ambas as camisas têm um caimento muito bom, mas elas se ajustam de maneiras diferentes ao corpo. A camisa de algodão tem um corte mais clássico, que proporciona um caimento estruturado e elegante, ideal para um look formal. Já a camisa de linho tem um caimento mais solto e fluido, o que pode trazer um toque mais descontraído, mas ainda assim sofisticado.\n",
      "\n",
      "Quanto aos tamanhos, temos disponíveis do P ao GG para ambas as opções. Se você quiser, posso ajudá-lo a encontrar o tamanho ideal para você. Você tem uma preferência de tamanho ou gostaria de experimentar alguma delas?\n",
      "\n",
      "Buyer (turno 5): Estou pensando nas duas opções. A camisa de algodão parece ser uma escolha segura para um evento formal, mas a de linho me atrai pela leveza. Eu gostaria de experimentar ambas para ver qual se ajusta melhor ao meu corpo. Você pode me ajudar a encontrar o tamanho ideal?\n",
      "\n",
      "Seller (turno 6): Claro! Fico feliz em ajudar você a experimentar as camisas. Vamos começar com a camisa de algodão. Ela custa R$ 129,90 e é feita de um tecido super confortável, ideal para um evento formal. A camisa de linho, por sua vez, custa R$ 149,90 e é perfeita para dias mais quentes, pois é leve e arejada.\n",
      "\n",
      "Para encontrar o tamanho ideal, você pode me dizer qual é o seu tamanho habitual ou se já experimentou camisas de outras marcas? Assim, posso ajudá-lo a escolher o tamanho mais adequado. Vou pegar as duas camisas para você experimentar. Você prefere começar pela de algodão ou pela de linho?\n",
      "\n",
      "Buyer (turno 7): Eu normalmente uso tamanho M, mas às vezes o tamanho G também pode ficar bom dependendo da marca. Você acha que eu deveria experimentar o M primeiro? E em relação ao ajuste, a camisa de linho pode ficar um pouco mais solta, certo? \n",
      "\n",
      "Vou experimentar primeiro a de algodão e depois a de linho. \n",
      "\n",
      "Vou levar.\n",
      "\n",
      "=== O Controller decidiu encerrar a conversa (STOP) ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4\")\n",
    "\n",
    "\n",
    "########################################\n",
    "# Memória Compartilhada\n",
    "########################################\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "########################################\n",
    "# BuyerChain\n",
    "########################################\n",
    "buyer_system_template = \"\"\"\n",
    "Você é o Buyer (comprador) em uma loja de roupas num shopping.\n",
    "Não se identifique como IA.\n",
    "\n",
    "Objetivo:\n",
    "- Você quer comprar alguma peça de roupa ou não.\n",
    "- Se ainda não decidiu, você pode fazer perguntas.\n",
    "- Quando decidir, diga explicitamente \"Vou levar\" ou \"Não vou levar\".\n",
    "  É SOMENTE COM ESSAS EXATAS PALAVRAS que você finaliza sua decisão.\n",
    "\n",
    "Histórico da conversa até agora:\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "buyer_human_template = \"\"\"\n",
    "A última fala do Vendedor (Seller) foi:\n",
    "{seller_utterance}\n",
    "\n",
    "Agora responda como BUYER:\n",
    "1. Se ainda estiver indeciso, faça perguntas ou diga o que acha.\n",
    "2. Para encerrar, diga EXATAMENTE \"Vou levar\" ou \"Não vou levar\".\n",
    "3. Não se identifique como IA.\n",
    "\"\"\"\n",
    "\n",
    "buyer_system_msg = SystemMessagePromptTemplate.from_template(buyer_system_template)\n",
    "buyer_human_msg = HumanMessagePromptTemplate.from_template(buyer_human_template)\n",
    "\n",
    "buyer_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"history\", \"seller_utterance\"],\n",
    "    messages=[buyer_system_msg, buyer_human_msg]\n",
    ")\n",
    "\n",
    "buyer_chain = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        openai_api_key=api_key,\n",
    "        model_name=model_name,\n",
    "        temperature=0.3\n",
    "    ),\n",
    "    prompt=buyer_prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "########################################\n",
    "# SellerChain\n",
    "########################################\n",
    "seller_system_template = \"\"\"\n",
    "Você é o Seller (vendedor) em uma loja de roupas num shopping.\n",
    "Não se identifique como IA.\n",
    "\n",
    "Objetivo:\n",
    "- Vender roupas, mas não encerrar a conversa. \n",
    "- Responda perguntas do Buyer.\n",
    "- Se o Buyer disser \"Vou levar\" ou \"Não vou levar\", é ele que está decidindo.\n",
    "\n",
    "Histórico da conversa até agora:\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "seller_human_template = \"\"\"\n",
    "A última fala do Comprador (Buyer) foi:\n",
    "{buyer_utterance}\n",
    "\n",
    "Agora responda como SELLER:\n",
    "1. Ofereça informações sobre as roupas, preços etc.\n",
    "2. Não se identifique como IA.\n",
    "3. Não encerre por conta própria.\n",
    "\"\"\"\n",
    "\n",
    "seller_system_msg = SystemMessagePromptTemplate.from_template(seller_system_template)\n",
    "seller_human_msg = HumanMessagePromptTemplate.from_template(seller_human_template)\n",
    "\n",
    "seller_prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"history\", \"buyer_utterance\"],\n",
    "    messages=[seller_system_msg, seller_human_msg]\n",
    ")\n",
    "\n",
    "seller_chain = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        openai_api_key=api_key,\n",
    "        model_name=model_name,\n",
    "        temperature=0.3\n",
    "    ),\n",
    "    prompt=seller_prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "########################################\n",
    "# ControllerChain\n",
    "########################################\n",
    "# Principais ajustes:\n",
    "# 1) Temperature=0.0 para ser literal.\n",
    "# 2) Prompt contendo EXEMPLOS de CONTINUE.\n",
    "# 3) Somente STOP se \"Vou levar\" ou \"Não vou levar\".\n",
    "\n",
    "controller_prompt_template = \"\"\"\n",
    "Você é um agente controlador que analisa o histórico da conversa (Buyer e Seller)\n",
    "e decide se deve ENCERRAR ou CONTINUAR.\n",
    "\n",
    "REGRAS:\n",
    "- Responda \"STOP\" SOMENTE se o Buyer disser EXATAMENTE \"Vou levar\" (sem nada antes ou depois) ou EXATAMENTE \"Não vou levar\".\n",
    "- Se houver qualquer condicionador ou complemento (ex: \"Acho que vou levar\", \"Se eu gostar, eu levo\", \"Vou levar se ficar bom\"), então é CONTINUE.\n",
    "- Qualquer outra fala => CONTINUE.\n",
    "- Não use nada além de STOP ou CONTINUE (sem aspas).\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "[CONTINUE] Buyer: \"Acho que vou levar, mas só depois de experimentar.\"\n",
    "[CONTINUE] Buyer: \"Se ficar bem, eu vou levar.\"\n",
    "[STOP] Buyer: \"Vou levar\"\n",
    "[STOP] Buyer: \"Não vou levar\"\n",
    "\n",
    "Leia o histórico abaixo e retorne somente STOP ou CONTINUE:\n",
    "\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "controller_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\"],\n",
    "    template=controller_prompt_template\n",
    ")\n",
    "\n",
    "controller_chain = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        openai_api_key=api_key,\n",
    "        model_name=model_name,\n",
    "        temperature=0.0  # Para seguir as instruções literalmente\n",
    "    ),\n",
    "    prompt=controller_prompt,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def should_stop_conversation() -> bool:\n",
    "    mem_vars = memory.load_memory_variables({})\n",
    "    msgs = mem_vars[\"history\"]\n",
    "    history_str = \"\"\n",
    "    for m in msgs:\n",
    "        role = \"Buyer\" if m.type == \"human\" else \"Seller\"\n",
    "        history_str += f\"{role}: {m.content}\\n\"\n",
    "\n",
    "    # (DEBUG opcional) Descomente se quiser ver o prompt real:\n",
    "    # print(\"=== Controller Prompt ===\")\n",
    "    # print(controller_prompt.format(history=history_str))\n",
    "\n",
    "    result = controller_chain.run({\"history\": history_str})\n",
    "    # (DEBUG opcional) print(\"Controller output:\", result)\n",
    "\n",
    "    return \"STOP\" in result.upper()\n",
    "\n",
    "\n",
    "########################################\n",
    "# Função principal\n",
    "########################################\n",
    "def main():\n",
    "    print(\"=== Conversa: Buyer vs Seller, com Controller ===\\n\")\n",
    "\n",
    "    buyer_utterance = input(\"Digite a FALA INICIAL do Buyer: \")\n",
    "    print()\n",
    "    memory.save_context({\"input\": \"\"}, {\"output\": buyer_utterance})\n",
    "    print(f\"Buyer (turno 1): {buyer_utterance}\\n\")\n",
    "\n",
    "    max_turns = 20\n",
    "    turn = 2\n",
    "    seller_utterance = \"\"\n",
    "\n",
    "    while turn <= max_turns:\n",
    "        if should_stop_conversation():\n",
    "            print(\"=== O Controller decidiu encerrar a conversa (STOP) ===\")\n",
    "            break\n",
    "\n",
    "        if turn % 2 == 0:\n",
    "            # Turno par => Seller\n",
    "            seller_response = seller_chain.run(buyer_utterance=buyer_utterance)\n",
    "            print(f\"Seller (turno {turn}): {seller_response}\\n\")\n",
    "            seller_utterance = seller_response\n",
    "        else:\n",
    "            # Turno ímpar => Buyer\n",
    "            buyer_response = buyer_chain.run(seller_utterance=seller_utterance)\n",
    "            print(f\"Buyer (turno {turn}): {buyer_response}\\n\")\n",
    "            buyer_utterance = buyer_response\n",
    "\n",
    "        turn += 1\n",
    "    else:\n",
    "        print(f\"=== A conversa atingiu {max_turns} turnos e foi encerrada ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
