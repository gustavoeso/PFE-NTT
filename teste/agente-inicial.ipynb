{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar variáveis do .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de multi-agentes guardando prompts anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Diálogo Gerado ===\n",
      "(Início da conversa)\n",
      "Comprador: Olá, tudo bem? Preciso de uma camisa branca para um casamento.\n",
      "\n",
      "Vendedor: Olá! Fico feliz em te ajudar a encontrar a camisa branca perfeita para o casamento. Tenho três ótimas opções que podem interessar:\n",
      "\n",
      "1. **Camisa de Algodão Premium** - Esta camisa é feita 100% em algodão egípcio, com um toque macio e um caimento excelente. É ideal para ocasiões formais. O preço é R$ 199,90.\n",
      "\n",
      "2. **Camisa Slim Fit** - Com um design moderno e corte ajustado, essa camisa é perfeita para quem busca um visual elegante e contemporâneo. O tecido é leve e respirável, ideal para o clima do casamento. Essa custa R$ 169,90.\n",
      "\n",
      "3. **Camisa Branca Clássica com Detalhes** - Esta opção traz um toque especial com detalhes sutis nas mangas e colarinho, sem perder a sofisticação. É um pouco mais casual, mas ainda assim elegante. O preço é R$ 149,90.\n",
      "\n",
      "Você gostaria de experimentar alguma dessas opções? Posso te ajudar a encontrar o tamanho certo!\n",
      "\n",
      "Comprador: Olá! As camisas parecem ótimas, mas eu estou buscando algo que não pese muito no bolso. Você consegue fazer um desconto se eu optar pela camisa clássica com detalhes por R$ 149,90? E quais tamanhos vocês têm disponíveis?\n",
      "\n",
      "Vendedor: Claro, entendo a sua preocupação com o preço! Posso oferecer um desconto e deixar a camisa branca clássica com detalhes por R$ 139,90. É uma ótima escolha para o casamento e vai garantir que você esteja elegante e confortável.\n",
      "\n",
      "Quanto aos tamanhos, temos disponível os seguintes: P, M, G e GG. Qual é o seu tamanho? Posso providenciar para você experimentar.\n",
      "\n",
      "Comprador: Ótimo! Agradeço pelo desconto, R$ 139,90 está um preço mais acessível. Eu uso tamanho M. Poderia me mostrar a camisa branca clássica com detalhes para eu experimentar?\n",
      "\n",
      "Vendedor: Claro! Vou buscar a camisa branca clássica com detalhes no tamanho M para você experimentar. Um momento, por favor. \n",
      "\n",
      "(Depois de alguns instantes)\n",
      "\n",
      "Aqui está a camisa! Veja como ela é elegante e os detalhes são muito sutis, tornando-a perfeita para o casamento. Você gostaria de experimentar agora? Se precisar de outros tamanhos ou quiser ver alguma outra opção, estou à disposição para ajudar!\n",
      "\n",
      "Comprador: Sim, gostaria de experimentar a camisa agora! Amei a oferta e o preço. Vou colocar e ver como fica. Também gostaria de saber sobre as formas de pagamento disponíveis e se você teria algum acerto no preço se eu levar mais de uma peça.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1) O codigo tem dois agentes: COMPRADOR e VENDEDOR\n",
    "comprador = Agent(\n",
    "    role=\"Comprador de Camisas\",\n",
    "    goal=\"Comprar uma camisa branca para um casamento. Já tem o terno, só precisa da camisa.\",\n",
    "    backstory=(\n",
    "        \"Você é um cliente que quer economizar; procura algo bonito e não muito caro. \"\n",
    "        \"Fale de forma breve, pergunte preços, negocie se possível.\"\n",
    "    ),\n",
    "    memory=False,\n",
    "    verbose=False,\n",
    "    allow_delegation=False,\n",
    "    llm=os.getenv(\"OPENAI_MODEL_NAME\") or \"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "vendedor = Agent(\n",
    "    role=\"Vendedor de Loja de Roupas\",\n",
    "    goal=\"Vender a melhor camisa branca possível para o cliente.\",\n",
    "    backstory=(\n",
    "        \"Você conhece bem o estoque. Procure oferecer 3 opções, discuta preços e tente concretizar a venda.\"\n",
    "    ),\n",
    "    memory=False,\n",
    "    verbose=False,\n",
    "    allow_delegation=False,\n",
    "    llm=os.getenv(\"OPENAI_MODEL_NAME\") or \"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "# 2) O codigo vai manter um histórico de conversas, como texto puro\n",
    "#    simulando um diálogo estilo:\n",
    "#      \"Comprador: ...\\nVendedor: ...\\nComprador: ...\\n\" assim os agentes quando recebem o histórico sabem quem falou o que\n",
    "historico = \"\"\"(Início da conversa)\n",
    "Comprador: Olá, tudo bem? Preciso de uma camisa branca para um casamento.\n",
    "\"\"\"\n",
    "\n",
    "def gerar_proxima_fala(agent: Agent, conversa: str, quem_fala: str) -> str:\n",
    "    \"\"\"\n",
    "    Cria uma Task para o 'agent' gerar a próxima fala no diálogo,\n",
    "    recebendo como contexto todo o 'conversa' (histórico) e\n",
    "    instruindo para falar como 'quem_fala'.\n",
    "    Retorna apenas o texto da fala (sem prefixos).\n",
    "    \"\"\"\n",
    "    # Monta o prompt de forma simples:\n",
    "    prompt = f\"\"\"\n",
    "Este é o histórico da conversa até agora:\n",
    "----------------\n",
    "{conversa}\n",
    "----------------\n",
    "\n",
    "Agora é a vez de {quem_fala} falar.\n",
    "Responda como se você fosse {quem_fala} no diálogo.\n",
    "Não faça rodeios sobre a própria IA, apenas continue a conversa.\n",
    "\"\"\"\n",
    "\n",
    "    # Cria a Task\n",
    "    t = Task(\n",
    "        description=prompt,\n",
    "        expected_output=f\"A próxima fala do {quem_fala} no diálogo.\",\n",
    "        agent=agent\n",
    "    )\n",
    "    # Executa via um Crew temporário\n",
    "    c = Crew(agents=[agent], tasks=[t], verbose=False)\n",
    "    result = c.kickoff()\n",
    "    return result.raw.strip()\n",
    "\n",
    "\n",
    "def executar_dialogo():\n",
    "    # Para demonstrar, vao ser feitas 6 trocas de falas (3 rodadas). 3 rodadas, cada 2 falas => total 6 falas\n",
    "    global historico\n",
    "\n",
    "    # Já começa com o comprador na conversa (definido acima).\n",
    "    # Vamos alternar: Vendedor -> Comprador -> Vendedor -> Comprador ...\n",
    "    for rodada in range(3):\n",
    "        # 1) Vendedor fala\n",
    "        fala_vendedor = gerar_proxima_fala(vendedor, historico, \"Vendedor\")\n",
    "        historico += f\"\\nVendedor: {fala_vendedor}\\n\"\n",
    "\n",
    "        # 2) Comprador fala\n",
    "        fala_comprador = gerar_proxima_fala(comprador, historico, \"Comprador\")\n",
    "        historico += f\"\\nComprador: {fala_comprador}\\n\"\n",
    "\n",
    "    # imprimimos todo o diálogo\n",
    "    print(\"=== Diálogo Gerado ===\")\n",
    "    print(historico)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    executar_dialogo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste multi-agentes 2 com memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Comprador:** Olá! Estou procurando uma camisa branca elegante para um casamento neste fim de semana. Você tem alguma opção disponível?  \n",
      "\n",
      "**Vendedor:** Olá! Sim, temos várias camisas brancas. Esta aqui é uma camisa de algodão de alta qualidade, perfeita para ocasiões formais. Ela custa R$ 250.  \n",
      "\n",
      "**Comprador:** Ela parece ótima, mas o preço está um pouco acima do que eu esperava. Você poderia fazer um desconto?  \n",
      "\n",
      "**Vendedor:** Entendo, mas essa camisa é realmente de alta qualidade. Posso oferecer um desconto de 10%, o que a deixaria por R$ 225.  \n",
      "\n",
      "**Comprador:** Agradeço a oferta, mas ainda assim está um pouco acima do meu orçamento. Se eu pagar em dinheiro, você poderia reduzir um pouco mais?  \n",
      "\n",
      "**Vendedor:** Se você pagar em dinheiro, posso baixar para R$ 210. É o melhor que posso fazer.  \n",
      "\n",
      "**Comprador:** R$ 210 é um preço mais justo, mas eu estava pensando em algo em torno de R$ 200. Você conseguiria fazer isso?  \n",
      "\n",
      "**Vendedor:** Hmmm, R$ 200 é um pouco apertado, mas eu realmente quero fechar essa venda. Que tal R$ 205?  \n",
      "\n",
      "**Comprador:** R$ 205 parece um bom compromisso. Fechado!  \n",
      "\n",
      "**Vendedor:** Ótimo! Vou embrulhar a camisa para você. Obrigado pela negociação!  \n",
      "\n",
      "**Comprador:** Eu que agradeço! Estou ansioso para usar a camisa no casamento.  \n",
      "\n",
      "**Vendedor:** Tenho certeza de que você ficará ótimo! Aqui está a sua camisa. Tenha um ótimo casamento!  \n",
      "\n",
      "**Comprador:** Obrigado! Até mais!\n"
     ]
    }
   ],
   "source": [
    "# Carregar variáveis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Usar as variáveis carregadas\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4-turbo\")  # Define um padrão caso não esteja no .env\n",
    "\n",
    "# Configuração do modelo de IA\n",
    "llm = ChatOpenAI(model_name=model_name, openai_api_key=api_key, temperature=0.3)\n",
    "\n",
    "# Memória resumida para manter o contexto sem gastar muitos tokens\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Definição dos agentes\n",
    "comprador = Agent(\n",
    "    role=\"Comprador\",\n",
    "    goal=\"Conversar com o vendedor e negociar diretamente o melhor preço para uma camisa branca de casamento.\",\n",
    "    backstory=\"Você tem um casamento neste fim de semana e precisa de uma camisa branca elegante. \"\n",
    "              \"Você quer pagar um preço justo e está pronto para negociar diretamente.\",\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "vendedor = Agent(\n",
    "    role=\"Vendedor de Loja de Roupas\",\n",
    "    goal=\"Interagir com o comprador de maneira dinâmica e negociar diretamente o preço de uma camisa branca.\",\n",
    "    backstory=\"Você trabalha em uma loja de roupas sofisticada e precisa vender camisas brancas. \"\n",
    "              \"Você responde diretamente ao comprador e apresenta os melhores argumentos para justificar o preço.\",\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Definição da tarefa principal (negociação de compra)\n",
    "task = Task(\n",
    "    description=(\n",
    "        \"O comprador e o vendedor devem conversar diretamente, alternando suas falas, para negociar a compra \"\n",
    "        \"de uma camisa branca para um casamento. O vendedor apresenta os produtos e o comprador negocia o preço. \"\n",
    "        \"A conversa deve ser um diálogo interativo e não uma narração.\"\n",
    "    ),\n",
    "    agent=comprador,  # Define um agente principal\n",
    "    memory=memory,\n",
    "    expected_output=\"Um diálogo interativo concluindo a negociação com um acordo entre os dois agentes.\"\n",
    ")\n",
    "\n",
    "# Criar a equipe com os dois agentes\n",
    "crew = Crew(\n",
    "    agents=[comprador, vendedor],\n",
    "    tasks=[task]\n",
    ")\n",
    "\n",
    "# Iniciar a conversa\n",
    "resultado = crew.kickoff()\n",
    "print(resultado)\n",
    "\n",
    "# Converter o resultado para string antes de salvar\n",
    "resultado_str = str(resultado)\n",
    "\n",
    "# Exportar conversa para um arquivo\n",
    "with open(\"conversa.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(resultado_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "def gerar_resposta(role, goal, memory, llm, mensagem_outro=None):\n",
    "    \"\"\"\n",
    "    Gera a resposta de um \"agente\" (sem classe) baseado no resumo da conversa\n",
    "    e na última mensagem do outro agente.\n",
    "    \"\"\"\n",
    "    # 1. Obtemos o histórico resumido armazenado na memória.\n",
    "    contexto = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "    # 2. Montamos o prompt para o LLM.\n",
    "    prompt = (\n",
    "        f\"Você é o {role}.\\n\"\n",
    "        f\"Seu objetivo: {goal}\\n\\n\"\n",
    "        f\"Contexto resumido da conversa até agora:\\n\"\n",
    "        f\"{contexto}\\n\\n\"\n",
    "    )\n",
    "    if mensagem_outro:\n",
    "        prompt += f\"O outro interlocutor disse: {mensagem_outro}\\n\"\n",
    "    prompt += \"Responda agora:\\n\"\n",
    "\n",
    "    # 3. Gera a resposta usando o LLM.\n",
    "    resposta = llm.predict(prompt)\n",
    "\n",
    "    # 4. Salvamos o turno (input/output) na memória:\n",
    "    #    \"input\": o que o outro agente falou,\n",
    "    #    \"output\": a resposta deste agente.\n",
    "    memory.save_context(\n",
    "        {\"input\": mensagem_outro if mensagem_outro else \"\"},\n",
    "        {\"output\": resposta}\n",
    "    )\n",
    "\n",
    "    return resposta\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Carrega variáveis do .env (ou configure manualmente)\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    model_name = os.getenv(\"OPENAI_MODEL_NAME\", \"gpt-4\")\n",
    "\n",
    "    # Configura LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        openai_api_key=api_key,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    # Cria memória resumida\n",
    "    memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "    # Definimos os dados dos \"agentes\" em dicionários:\n",
    "    comprador = {\n",
    "        \"role\": \"Comprador\",\n",
    "        \"goal\": \"Conversar com o vendedor e negociar diretamente o melhor preço para uma camisa branca de casamento.\"\n",
    "    }\n",
    "    vendedor = {\n",
    "        \"role\": \"Vendedor\",\n",
    "        \"goal\": \"Vender a camisa branca e justificar o preço.\"\n",
    "    }\n",
    "\n",
    "    # Vamos fazer 6 trocas de mensagem (3 rodadas de cada lado).\n",
    "    num_trocas = 6\n",
    "    mensagem_atual = None\n",
    "\n",
    "    for i in range(num_trocas):\n",
    "        # Decide se é a vez do comprador ou do vendedor:\n",
    "        if i % 2 == 0:  \n",
    "            # Vez do comprador\n",
    "            role = comprador[\"role\"]\n",
    "            goal = comprador[\"goal\"]\n",
    "        else:\n",
    "            # Vez do vendedor\n",
    "            role = vendedor[\"role\"]\n",
    "            goal = vendedor[\"goal\"]\n",
    "\n",
    "        # Gera resposta para o turno atual\n",
    "        resposta = gerar_resposta(role, goal, memory, llm, mensagem_atual)\n",
    "        \n",
    "        # Imprime e prepara para o próximo turno\n",
    "        print(f\"{role}: {resposta}\\n\")\n",
    "        mensagem_atual = resposta\n",
    "\n",
    "    # Ao final, se quiser salvar o resumo (ou a conversa final) em um arquivo:\n",
    "    resumo_final = memory.load_memory_variables({})[\"history\"]\n",
    "    with open(\"conversa.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(resumo_final)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
